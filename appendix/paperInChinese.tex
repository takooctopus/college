% !Mode:: "TeX:UTF-8"

\titlecontents{chapter}[2em]{\vspace{.5\baselineskip}\xiaosan\song}
             {\prechaptername\CJKnumber{\thecontentslabel}\postchaptername\qquad}{}
             {}             % 设置该选项为空是为了不让目录中显示页码
\addcontentsline{toc}{chapter}{中文译文}
\setcounter{page}{1}            % 单独从 1 开始编页码
\markboth{中文译文}{中文译文}   % 用于将章节号添加到页眉中
\chapter*{中文译文}

\trtitle{图像识别的深层残差网络}

\trchapter{介绍}

深层残差网路对于图像分类问题已经有了很多的突破，深层网络通常会整合低\\中\\高三中不同的层特征，并将其分类到多种层方法上，由此，层特征可以通过堆叠层的数量增多而得到增强。最近的研究表明了网络的深度很重要，而最新的研究中，针对ImageNet的深层网络达到了从16层到30层。其他很多平凡的视觉检测任务也在深层模型中有很对的益处。

被网络深度的优点驱动，我们会得到一个问题：是否随着堆叠层的层数增多，其学习的效率就会越高嘛？很重要的一个问题就是梯度消失。正太分布的参数初始化和中间标准化层已经有很多显示这个问题了，其能允许有着数层的网络能够为反向传播的随机的梯度下降产生收敛的效果。

当更深的网络开始收敛时，梯度消失就暴露了出来：随着网络深度增加，准确率开始饱和，真实缓慢降低。这种降低并非是由过拟合造成的，添加更多的层会导致更大的训练误差，正如之前的成果表明的那样，我们的实验也证明了这一点。

训练准确率的降低暗示了并非所有的系统都能这么简单的去优化。让我们考虑一个更为浅层的网络结构，并且其更深层的副本会增加更多的层。我们生层深层模型时有一种结对方法：添加的层是其他层的相等映射，即其是浅层模型层的拷贝。这种方法揭示了深层模型不应该产生比浅层模型更少的训练误差。但是实验显示我们现在的解决方法并不能找到一个相比于生成方案更为合适的方法。

在这篇文章中，我们引入了深层残差学习框架，并非期望少数堆叠层能直接适应于底层映射，我们使用残差映射。我们一般的将其映射定义为 $ H(x) $，我们将堆叠的非线性层匹配于另一个映射定义为 $ F(x) = H(x) -x $ ， 原来的映射被重定义为 $ F(x)+x $。我们假设相比于原来的网络，残差映射能够更好的优化。如果一个恒等映射被最优化了，则会更容易将残差优化到0。

我们的公式 $ F(x)+x $ 可以看作一个捷径链接，其会跳过一到多个层。在我们的情况下，这种捷径连接简单的使用了恒等映射技术，并且他们的输出被简单的加到了堆叠层中。恒等捷径连接并未添加了额外的参数或是额外的复杂度。整个网络仍接可以通过梯度下降训练反向传播，也能通过共同库来进行实现。

我们在ImageNet上的实验显示了梯度消失问题并验证了我们的方法，实验显示：1)我们的残差网络能够很简单地进行优化，但简单网络有更大的误差。2)我们的残差网络能够很简单通过增加深度获得准确的增益。

相似的现象也在CIFAR-10数据集上产生了，暗示了优化难度以及我们的方法并非限制于特定的数据集，我们在超过100层的数据集甚至于超越1000层的网络中有很好的训练效果。

在ImageNet分类数据集中，我们也有着很好的结果。我们的152层残差网络是已知的针对这个最深的网络了。也仍旧比VGG网络有更好的效果。在ImageNet测试集上我们有3.75\%的top5误差，并获得了ILSVRC2015的分类比赛冠军。在其他识别问题上这个也获得了很好的效果，并让我们获得了很多其他的冠军。

\trchapter{相关工作}

\trsection{残差表示}

在图像识别的领域里，VLAD是通过残差响亮编码的表示，Fisher向量能被格式化为VLAD的概率化版本。他们都是对于图像检索和分类都很有效。对于矢量量化，编码残差向量显示出比编码原始向量更有效。在低级视觉和计算机图形学中，用于求解偏微分方程被广泛使用多重网格方法，将系统重新制定为多个尺度的子问题，其中每个子问题负责较粗和较精细之间的残差解。Multigrid的替代方法是层次基础预处理，它依赖于表示两个尺度之间的残差向量的变量。这些求解器比不知道解的残余性质的标准求解器收敛得快得多。 这些方法表明良好的重构或预处理可以简化优化过程。

\trsection{捷径连接}

长期以来，人们一直在研究导致捷径连接的实践和理论。训练多层感知器（MLP）的早期实践是添加从网络输入连接到输出的线性层。 一些中间层直接连接到辅助分类器，以解决消失/爆炸梯度。 这些论文提出了通过捷径连接实现的中心层响应，渐变和传播错误的方法。 “初始”层由捷径分支和几个更深的分支组成。

与我们的工作相比，“高速公路网络”提供与门控功能的捷径连接。 与我们的无参数身份捷径方式相比，这些门是数据相关的并且具有参数。当门控捷径方式“关闭”（接近零）时，公路网络中的层表示非残差功能。 相反，我们的表述总是学习剩余的功能; 我们的恒等捷径方式永远不会关闭，所有信息都会通过，并且我们还需要学习额外的残差功能。但此外，高速网络在深度增加极大的同时并没有显着提高精度。

\trchapter{深层残差学习}

\trsection{残差学习}

让我们将$ H（x）$ 视为底层映射，使其适合几个堆叠层（不一定是整个网络），$x$表示这些层中第一层的输入。 如果假设多个非线性层可以渐近逼近复杂函数，那么它等效于假设它们可以渐近逼近残差函数，即$H（x）-x$（假设输入和输出具有相同的维度）。 因此，我们明确地让这些层近似于残差函数$F（x）：= H（x） -  x$，而不是期望堆叠层接近$H（x）$。 因此原始函数变为$F（x）+ x$。 虽然两种形式都应该能够渐近地逼近所需的函数，但学习的难度可能会有所不同。

产生这种重格式化的动机是由于退化问题的反直觉的现象。正如在引言中所讨论的，如果添加的层可以构造为恒等映射，则更深的模型应该不具有比其较浅的对应物更大的训练误差。退化问题表明，求解器可能难以通过多个非线性层来近似同一性映射。利用残差学习重构，如果恒等映射是最优的，则求解器可以简单地将多个非线性层的权重推向零以接近恒等映射。

在实际情况下，恒等映射不太可能是最优的，但我们的重构可能有助于预先解决问题。如果最优函数更接近于恒等映射而不是零映射，则求解器应该更容易参考恒等映射来查找扰动，而不是将该函数作为新映射来学习。我们通过实验表明，学习的残差函数通常具有较小的响应，这表明恒等映射提供了合理的预处理。

\trsection{基于捷径的恒等映射}

我们每个堆叠层都采用残差学习。构建块如图2所示。在本文中，我们定义构建块：

\begin{eqnarray}
    y = F(x,\{W_i\})+x
    \label{euq:tran-1}
\end{eqnarray}

这里 $ x $ 和 $ y $ 是输入和输出向量。函数 $ F(x,\{W_i\}) $ 表示了要被学习的残差映射。比方说两层 $ F = W_2 \sigma ( W_1 x) $，这里 $ \sigma $ 表示了线性整流函数，我们还省略了偏置以简化符号。通过捷径连接和逐元素添加来执行操作 $ F+x $ 。在加法后我们采用第二非线性。

方程\ref{euq:tran-1}中的捷径方式连接既不引入额外参数也不引入计算复杂性。 这不仅在实践中具有吸引力，而且在我们对普通网络和剩余网络之间的比较中也很重要。 我们可以公平地比较同时具有相同数量的参数、深度、宽度和计算成本的普通/残差网络（除开可忽略的元素添加）。

公式\ref{euq:tran-1}中 $ x $ 和 $ F $ 的尺寸必须相等。 如果不是这种情况（比方说在更改输入/输出通道时），我们可以通过捷径方式连接执行线性投影$ W_s $以匹配尺寸：

\begin{eqnarray}
    y = F(x,\{W_i\}) + W_s x
    \label{euq:tran-2}
\end{eqnarray}

我们也可以在方程\ref{euq:tran-1}中使用矩阵 $W_s$。 但是我们将通过实验证明，恒等映射足以解决退化问题，并且它是经济的，因此 $W_s$一般只在匹配维度时使用。

残差函数$F$的形式是灵活的。本文中的实验涉及具有两层或三层的函数$F$，而更多层也并非不可。 但是如果$F$只有一层，则方程\ref{euq:tran-1}类似于线性层：$y = W_1x + x$，这样就没有啥优点了。

我们还注意到，尽管为简单起见，上述符号是基于全连接层，但它们也适用于卷积层。函数$F（x,\{W_i\}）$可以表示多个卷积层。并两个特征映射上逐个通道执行逐元素的添加。

\trsection{网络结构}

我们测试了各种普通/残差网洛，并观察到了一致的现象。为了提供讨论的实例，我们为ImageNet描述了两个模型，如下所示。

\trsubsection{简单网络}

我们的简单网络主要受到VGG网络的启发。卷积层大多具有 $ 3\times3 $滤波器并遵循着两个简单的设计规则：（i）对于相同的输出特征映射大小，这些层具有相同数量的滤波器; （ii）如果特征映射大小减半，则过滤器的数量加倍，以便保持每层的时间复杂度。 我们直接通过步幅为2的卷积层进行下采样。网络最后以全局平均池层和带有softmax的1000路全连接层结束。且加权层的总数是34。

值得注意的是，我们的模型比VGG网络具有更少的过滤器和更低的复杂性。我们的34层简单网络有36亿FLOP（乘法增加），仅为VGG-19（196亿FLOP）的18％。

\trsubsection{残差网络}

基于上述普通网络，我们插入了捷径连接，将网络转换为对应的残差版本。 当输入和输出具有相同的尺寸时，可以直接使用标识捷径方式（公式\ref{euq:tran-1}）（图3中的实线快捷方式）。 当尺寸增加时（图3中的虚线快捷键），我们考虑两个选项：（A）捷径方式仍然执行恒等映射，为增加尺寸填充额外的零条目。此选项不引入额外参数;（B）方程（2）中的投影捷径方式用于匹配尺寸（由$1\times1$卷积完成）。 对于这两个选项，当捷径方式跨越两种尺寸的特征图时，它们的步幅为2。

\trsection{实现}

我们对ImageNet的实现遵循[21,41]中的实践，我们调整图像大小，其短边在 $[256,480]$ 中随机采样用于尺度增强[41]。从图像或其水平翻转中随机进行 $224\times224$ 裁剪采样，减去每像素平均值[21]。使用[21]中的标准颜色增强。 我们在每次卷积之后和激活之前采用批量归一化（BN）[16]。再之后，我们在[13]中初始化权重，并从头开始训练所有普通/残差网洛。我们使用小批量256的SGD。学习率从0.1开始，当误差平稳时再除以10，并且模型训练最多$60\times10^4$次迭代。我们使用$0.0001$的衰减系数和$0.9$的动量系数。按照[16]中的实践，我们不使用dropout[14]。

在测试时，出于比较研究，我们采用标准的10种作物测试[21]。为了获得最佳结果，我们采用[41,13]中的全卷积形式，并在多个尺度上平均获得得分（图像被调整大小，使得短边在$\{224; 256; 384; 480; 640\}$ 之中）。

\trchapter{实验结果}

\trsection{ImageNet分类}

我们在由1000个类组成的ImageNet-2012分类数据集上评估我们的方法。模型在128万个训练图像上进行训练，并在50k验证图像上进行评估。我们还获得了测试服务器报告的100k测试图像的最终结果。我们评估top-1和top-5的错误率。

\trsubsection{简单网络}

我们首先评估18层和34层简单网。34层普通网在图3（中间）。18层普通网具有类似的形式。

表2中的结果表明，较深的34层简单网络比较浅的18层简单网络具有更高的验证误差。为了揭示其中原因，在图4（左）中我们比较了他们在训练过程中的训练/验证错误。我们观察到了退化问题——即使18层简单网络的解空间是34层简单网络的子空间，34层普通网在整个训练过程中也有较高的训练误差。

我们认为这种优化难度不太可能是由于梯度消失造成的。这些普通网络采用BN[16]进行训练，确保前向传播信号具有非零方差。我们还验证了向后传播的梯度与BN表现正常。因此，前向和后向信号都不会消失。事实上，34层普通网仍然能够达到有竞争力的准确度（表3），这表明求解器在某种程度上起作用。 我们推测深层简单网可能具有指数级的低收敛速度，这会影响训练误差的减少。将来将研究这种优化困难的原因。

\trsubsection{残差网络}

我们评估了18层和34层残差网洛（ResNets）。基础架构与上述简单网络相同，期望在每对 $3 \times 3$滤波器中添加快捷连接，如图3（右）所示。 在第一次比较中（表2和图4右），我们对所有捷径方式使用恒等映射，为增加维度使用零填充（选项A）。 因此，与普通同行相比，他们也没有额外的参数。

我们从表2和图4中得到了三个主要观察结果。首先，情况与残差学习相反——34层ResNet优于18层ResNet（2.8％）。更重要的是，34层ResNet表现出相当低的训练误差，并且可以推广到验证数据。 这表明在该设置中很好地解决了退化问题，并且我们设法从增加的深度获得准确性上的增益。

其次，与普通对应物相比，由于成功减少了训练误差（图4右对左），34层的ResNet将前1个误差减少了3.5％（表2）。该比较验证了极深系统上残差学习的有效性。

最后，我们还注意到18层简单/残留网络是相对准确的（表2），但18层ResNet收敛速度更快（图4右侧与左侧）。当网络“不太深”（此处为18层）时，当前的SGD求解器仍然能够找到简单网络的良好解决方案。 在这种情况下，ResNet通过在早期阶段提供更快的收敛来简化优化。

\trsubsection{恒等和投影的比较}

我们已经证明，无参数的恒等捷径方式有助于训练。接下来我们研究投影捷径方式（方程（2））。 在表3中，我们比较了三个选项：（A）零填充快捷方式用于增加尺寸，所有捷径方式都是无参数（与表2和图4相同）; （B）投影快捷方式用于增加尺寸，其他快捷方式是标识; （C）所有捷径都是预测。

表3显示所有三个选项都明显优于简单网络对应选项。B稍微好于A.我们认为这是因为A中的零填充维度确实没有残差学习。C略微优于B，我们将其归因于许多（13个）投影捷径方式引入的额外参数。但A/B/C之间的微小差异表明，预测捷径对解决退化问题并不重要。因此，我们在本文的其余部分不使用选项C来减少内存/时间复杂度和模型大小。恒等捷径方式对于不增加瓶颈架构的复杂性特别重要。
